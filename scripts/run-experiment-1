#!/usr/bin/env python3
'''
Compare the accuracy and efficiency of decision smoothing and cost regression
networks.
'''
from json import dumps
from os import makedirs

import tensorflow as tf

from lib.data import Dataset
from lib.layers import (
    BatchNorm, Chain, Conv, CrossEntropyError, Layer, LinTrans, MaxPool,
    MultiscaleConvMax, Rect, SelectPyramidTop, Softmax, SquaredError, ToPyramid)
from lib.nets import CRNet, DSNet, SRNet
from lib.training import train

################################################################################
# Load data.
################################################################################

dataset = Dataset('data/cifar-10-lln.mat', n_vl=1280)
x0_shape = dataset.x0_shape
y_shape = dataset.y_shape

################################################################################
# Define network hyperparameters.
################################################################################

k_cpts = [3e-9]# + [4**i * 1e-10 for i in range(7)]
k_l2 = 1e-3

################################################################################
# Allocate a log for the experiment results.
################################################################################

log = {'sr': [], 'ds_casc': [], 'cr_casc': [],  'ds_tree': [],  'cr_tree': []}

################################################################################
# Define network components.
################################################################################

class Input(Layer):
    pass

class ReConvMP(Chain):
    def __init__(self, n_chan):
        conv_hypers = dict(n_chan=n_chan, supp=3, k_l2=k_l2, σ_w=1e-2)
        super().__init__(
            Conv(**conv_hypers), BatchNorm(), Rect(),
            Conv(**conv_hypers), BatchNorm(), Rect(),
            Conv(**conv_hypers), BatchNorm(), Rect(),
            MaxPool(stride=2, supp=2))

class LogReg(Chain):
    def __init__(self):
        super().__init__(
            LinTrans(n_chan=y_shape[0], k_l2=k_l2, σ_w=1e-2),
            Softmax(), SquaredError())

class ReConvMPMS(Chain):
    def __init__(self, scale0, n_scales, n_chan):
        super().__init__(
            MultiscaleConvMax(
                scale0=scale0, n_scales=n_scales, n_chan=n_chan,
                supp=3, k_l2=k_l2, σ_w=1e-2),
            BatchNorm(), Rect())

class LogRegMS(Chain):
    def __init__(self, scale0):
        super().__init__(
            SelectPyramidTop(scale0=scale0),
            LinTrans(n_chan=y_shape[0], k_l2=k_l2, σ_w=1e-2),
            Softmax(), SquaredError())

################################################################################
# Define an optimizer.
################################################################################

t_train = tf.placeholder(tf.float32, ())
λ_learn = 0.1 / 2**(t_train / 10)
optimizer = tf.train.MomentumOptimizer(λ_learn, 0.9)

################################################################################
# Train a statically-routed network.
################################################################################

print('\n —— Training statically-routed networks... ——\n')

for n_tf in [3]:
    root = LogReg()
    for i in reversed(range(n_tf)):
        root = [ReConvMP(32 * 2**i), root]
    root = [Input(), root]
    net = SRNet(x0_shape, y_shape, optimizer, root)
    # net = SRNet(x0_shape, y_shape, optimizer,
    #     [Input(),
    #         [ToPyramid(n_scales=4),
    #             [ReConvMPMS((32, 32), 4, 32),
    #             [ReConvMPMS((32, 32), 4, 32),
    #             [ReConvMPMS((32, 32), 4, 32),
    #                 [ReConvMPMS((32, 32), 3, 64),
    #                 [ReConvMPMS((16, 16), 3, 64),
    #                 [ReConvMPMS((16, 16), 3, 64),
    #                     [ReConvMPMS((16, 16), 2, 128),
    #                     [ReConvMPMS((8, 8), 2, 128),
    #                     [ReConvMPMS((8, 8), 2, 128),
    #                         LogRegMS((8, 8))]]]]]]]]]]])
    name = 'Static Routing (n_tf=%i)' % n_tf
    desc = train(net, dataset, logging_period=1, name=name, hypers=(
        lambda t: {t_train: t}))
    log['sr'].append({'n_tf': n_tf, 'net': desc})

################################################################################
# Train decision smoothing cascades.
################################################################################

print('\n —— Training decision smoothing cascades... ——\n')

for k_cpt in k_cpts:
    hypers = dict(k_cpt=k_cpt, k_l2=k_l2)
    net = DSNet(x0_shape, y_shape, optimizer, hypers,
        [Input(), LogReg(),
            [ReConvMP(32), LogReg(),
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()]]]])
    name = 'Decision Smoothing Cascade (k_cpt=%.1e)' % k_cpt
    desc = train(net, dataset, logging_period=1, name=name, hypers=(
        lambda t: {t_train: t}))
    log['ds_casc'].append({'k_cpt': k_cpt, 'net': desc})

################################################################################
# Train cost regressing cascades.
################################################################################

print('\n —— Training cost regression cascades... ——\n')

for k_cpt in k_cpts:
    hypers = dict(k_cpt=k_cpt, k_l2=k_l2, optimistic=True)
    net = CRNet(x0_shape, y_shape, optimizer, hypers,
        [Input(), LogReg(),
            [ReConvMP(32), LogReg(),
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()]]]])
    name = 'Cost Regression Cascade (k_cpt=%.1e)' % k_cpt
    desc = train(net, dataset, logging_period=1, name=name, hypers=(
        lambda t: {t_train: t}))
    log['cr_casc'].append({'k_cpt': k_cpt, 'net': desc})

################################################################################
# Train decision smoothing trees.
################################################################################

print('\n —— Training decision smoothing trees... ——\n')

for k_cpt in k_cpts:
    hypers = dict(k_cpt=k_cpt, k_l2=k_l2)
    net = DSNet(x0_shape, y_shape, optimizer, hypers,
        [Input(), LogReg(),
            [ReConvMP(32), LogReg(),
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()],
                    [ReConvMP(128), LogReg()]],
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()],
                    [ReConvMP(128), LogReg()]]],
            [ReConvMP(32), LogReg(),
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()],
                    [ReConvMP(128), LogReg()]],
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()],
                    [ReConvMP(128), LogReg()]]]])
    name = 'Decision Smoothing Tree (k_cpt=%.1e)' % k_cpt
    desc = train(net, dataset, logging_period=1, name=name, hypers=(
        lambda t: {t_train: t}))
    log['ds_tree'].append({'k_cpt': k_cpt, 'net': desc})

################################################################################
# Train cost regressing trees.
################################################################################

print('\n —— Training cost regression trees... ——\n')

for k_cpt in k_cpts:
    hypers = dict(k_cpt=k_cpt, k_l2=k_l2, optimistic=True)
    net = CRNet(x0_shape, y_shape, optimizer, hypers,
        [Input(), LogReg(),
            [ReConvMP(32), LogReg(),
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()],
                    [ReConvMP(128), LogReg()]],
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()],
                    [ReConvMP(128), LogReg()]]],
            [ReConvMP(32), LogReg(),
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()],
                    [ReConvMP(128), LogReg()]],
                [ReConvMP(64), LogReg(),
                    [ReConvMP(128), LogReg()],
                    [ReConvMP(128), LogReg()]]]])
    name = 'Cost Regression Tree (k_cpt=%.1e)' % k_cpt
    desc = train(net, dataset, logging_period=5, name=name, hypers=(
        lambda t: {t_train: t}))
    log['cr_tree'].append({'k_cpt': k_cpt, 'net': desc})

################################################################################
# Save the results.
################################################################################

makedirs('nets/', exist_ok=True)
with open('nets/experiment-1.json', 'w') as f:
    f.write(dumps(log, sort_keys=True, indent=2, separators=(',', ': ')))

print('\n —— Saved the results as `nets/experiment-1.json`. ——\n')
