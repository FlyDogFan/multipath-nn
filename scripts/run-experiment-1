#!/usr/bin/env python3
'''
Compare the accuracy and efficiency of decision smoothing and cost regression
networks.
'''
from json import dumps
from os import makedirs

import tensorflow as tf

from lib.data import Dataset
from lib.layers import BatchNorm, Chain, Conv, LinTrans, MaxPool, Rect
from lib.layers import LogReg as LibLogReg
from lib.nets import CRNet, DSNet, SRNet
from lib.training import train

################################################################################
# Load data.
################################################################################

dataset = Dataset('data/cifar-10-lln.mat')
x0_shape = dataset.x0_shape
y_shape = dataset.y_shape

################################################################################
# Define network hyperparameters.
################################################################################

k_cpts = [1e-9]# + [4**i * 1e-10 for i in range(7)]
k_l2 = 1e-3

################################################################################
# Allocate a log for the experiment results.
################################################################################

log = {'sr': [], 'ds': [], 'cr': []}

################################################################################
# Define network components.
################################################################################

class ReConvMP(Chain):
    def __init__(self, n_chan, *sinks):
        conv_params = {'n_chan': n_chan, 'supp': 3, 'k_l2': k_l2}
        pool_params = {'stride': 2, 'supp': 2}
        super().__init__({}, [
            Conv(conv_params), BatchNorm({}), Rect({}),
            Conv(conv_params), BatchNorm({}), Rect({}),
            Conv(conv_params), BatchNorm({}), Rect({}),
            MaxPool(pool_params)], *sinks)

class ReLin(Chain):
    def __init__(self, n_chan, *sinks):
        super().__init__({}, [
            LinTrans({'n_chan': n_chan, 'k_l2': k_l2}),
            BatchNorm({}), Rect({})], *sinks)

class LogReg(Chain):
    def __init__(self):
        super().__init__({}, [LibLogReg({'k_l2': k_l2})])

################################################################################
# Define an optimizer.
################################################################################

t_train = tf.placeholder(tf.float32, ())
λ_learn = 0.1 / 2**(t_train / 10)
optimizer = tf.train.MomentumOptimizer(λ_learn, 0.9)

################################################################################
# Train a statically-routed network.
################################################################################

# print('\n —— Training statically-routed networks... ——\n')
#
# for n_tf in [3]:
# # for n_tf in range(4):
#     root = ReLin(512, LogReg())
#     for i in reversed(range(n_tf)):
#         root = ReConvMP(64 * 2**i, root)
#     net = SRNet(x0_shape, y_shape, optimizer, root)
#     name = 'Static Routing (n_tf=%i)' % n_tf
#     desc = train(net, dataset, name=name, hypers=(lambda t: {t_train: t}))
#     log['sr'].append({'n_tf': n_tf, 'net': desc})

################################################################################
# Train decision smoothing cascades.
################################################################################

# print('\n —— Training decision smoothing cascades... ——\n')
#
# for k_cpt in k_cpts:
#     net = DSNet(x0_shape, y_shape, optimizer,
#         ReConvMP(64, LogReg(),
#             ReConvMP(128, LogReg(),
#                 ReConvMP(256, LogReg(),
#                     ReLin(512, LogReg())))))
#     name = 'Decision Smoothing Cascade (k_cpt=%.1e)' % k_cpt
#     desc = train(net, dataset, logging_period=1, name=name, hypers=(
#         lambda t: {net.k_cpt: k_cpt, net.k_l2: k_l2, t_train: t}))
#     log['ds'].append({'k_cpt': k_cpt, 'net': desc})

################################################################################
# Train cost regressing cascades.
################################################################################

print('\n —— Training cost regression cascades... ——\n')

for k_cpt in k_cpts:
    net = CRNet(x0_shape, y_shape, optimizer,
        ReConvMP(64, LogReg(),
            ReConvMP(128, LogReg(),
                ReConvMP(256, LogReg(),
                    ReLin(512, LogReg())))))
    name = 'Cost Regression Cascade (k_cpt=%.1e)' % k_cpt
    desc = train(net, dataset, logging_period=1, name=name, hypers=(
        lambda t: {net.k_cpt: k_cpt, net.k_l2: k_l2, t_train: t}))
    log['cr'].append({'k_cpt': k_cpt, 'net': desc})

################################################################################
# Train decision smoothing trees.
################################################################################

print('\n —— Training decision smoothing trees... ——\n')

################################################################################
# Train cost regressing trees.
################################################################################

print('\n —— Training cost regression trees... ——\n')

################################################################################
# Save the results.
################################################################################

makedirs('nets/', exist_ok=True)
with open('nets/experiment-1.json', 'w') as f:
    f.write(dumps(log, sort_keys=True, indent=2, separators=(',', ': ')))

print('\n —— Saved the results as `nets/experiment-1.json`. ——\n')
