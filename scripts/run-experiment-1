#!/usr/bin/env python3
'''
Compare the accuracy and efficiency of decision smoothing and cost regression
networks.
'''
from json import dumps
from os import makedirs

import tensorflow as tf

from lib.data import Dataset
from lib.layers import BatchNorm, Conv, LinTrans, LogReg, MaxPool, Rect
from lib.nets import CRNet, DSNet, SRNet
from lib.training import train

################################################################################
# Load data.
################################################################################

dataset = Dataset('data/cifar-10-lln.mat')
x0_shape = dataset.x0_shape
y_shape = dataset.y_shape
n_cls = y_shape[0]

################################################################################
# Define network hyperparameters.
################################################################################

k_cpts = [0]# + [4**i * 1e-10 for i in range(7)]
k_l2 = 1e-3
λ = 1

batch_size = 64
n_epochs = 100

################################################################################
# Allocate a log for the experiment results.
################################################################################

log = {'sr': [], 'ds': [], 'cr': []}

################################################################################
# Define network components.
################################################################################

def re_conv_mp(n_chan, *sinks):
    conv_params = {'n_chan': n_chan, 'supp': 3, 'k_l2': k_l2}
    pool_params = {'stride': 2, 'supp': 2}
    return (
        Conv(conv_params, BatchNorm({}, Rect({},
            Conv(conv_params, BatchNorm({}, Rect({},
                Conv(conv_params, BatchNorm({}, Rect({},
                    MaxPool(pool_params, *sinks)))))))))))

def re_lin(n_chan, *sinks):
    return LinTrans({'n_chan': n_chan, 'k_l2': k_l2},
        BatchNorm({}, Rect({}, *sinks)))

def log_reg():
    return LogReg({'k_l2': k_l2})

################################################################################
# Define an optimizer.
################################################################################

t_train = tf.placeholder(tf.float32, ())
λ_learn = 0.1 / 2**(t_train / 10)
optimizer = tf.train.MomentumOptimizer(λ_learn, 0.9)

################################################################################
# Train a statically-routed network.
################################################################################

# print('\n —— Training statically-routed networks... ——\n')
#
# for n_tf in range(4):
#     root = re_lin(512, log_reg())
#     for i in reversed(range(n_tf)):
#         root = re_conv_mp(64 * 2**i, root)
#     net = SRNet(x0_shape, y_shape, root)
#     name = 'Static Routing (n_tf=%i)' % n_tf
#     desc = train(net, dataset, optimizer, name=name, params=(
#         lambda t: {t_train: t}))
#     log['sr'].append({'n_tf': n_tf, 'net': desc})

################################################################################
# Train decision smoothing cascades.
################################################################################

print('\n —— Training decision smoothing cascades... ——\n')

for k_cpt in k_cpts:
    net = DSNet(x0_shape, y_shape,
        re_conv_mp(64, log_reg(),
            re_conv_mp(128, log_reg(),
                re_conv_mp(256, log_reg(),
                    re_lin(512, log_reg())))))
    name = 'Decision Smoothing Cascade (k_cpt=%.1e)' % k_cpt
    desc = train(net, dataset, optimizer, logging_period=5, name=name, params=(
        lambda t: {net.k_cpt: k_cpt, net.k_l2: k_l2, t_train: t}))
    log['ds'].append({'k_cpt': k_cpt, 'net': desc})

################################################################################
# Train cost regressing cascades.
################################################################################

# print('\n —— Training cost regression cascades... ——\n')
#
# for k_cpt in k_cpts:
#     net = CRNet(x0_shape, y_shape,
#         re_conv_mp(64, log_reg(),
#             re_conv_mp(128, log_reg(),
#                 re_conv_mp(256, log_reg(),
#                     re_lin(512, log_reg())))))
#     name = 'Cost Regression Cascade (k_cpt=%.1e)' % k_cpt
#     desc = train(net, dataset, optimizer, logging_period=1, name=name, params=(
#         lambda t: {net.k_cpt: k_cpt, net.k_l2: k_l2, t_train: t}))
#     log['cr'].append({'k_cpt': k_cpt, 'net': desc})

################################################################################
# Train decision smoothing trees.
################################################################################

print('\n —— Training decision smoothing trees... ——\n')

################################################################################
# Train cost regressing trees.
################################################################################

print('\n —— Training cost regression trees... ——\n')

################################################################################
# Save the results.
################################################################################

makedirs('nets/', exist_ok=True)
with open('nets/experiment-1.json', 'w') as f:
    f.write(dumps(log, sort_keys=True, indent=2, separators=(',', ': ')))

print('\n —— Saved the results as `nets/experiment-1.json`. ——\n')
