#!/usr/bin/env python3
'''
Train dynamically-routed networks that can adapt to computation-cost variations.
'''
from argparse import ArgumentParser
from os import makedirs
from types import SimpleNamespace as Ns

import numpy as np
import numpy.random as rand
import tensorflow as tf

from lib.data import Dataset
from lib.desc import net_desc
from lib.serdes import write_net
from arch_and_hypers import (
    arch, batch_size, cr_chain, cr_tree, ds_chain, ds_tree, k_cpts, n_iter,
    sr_chain, t_log, λ_lrn, τ_cr, τ_ds)

################################################################################
# Define experiments.
################################################################################

ds_hypers_dynkcpt = lambda net, t: {
    net.τ: τ_ds(t), net.k_cpt: rand.choice(k_cpts, batch_size)}

experiments = {
    'ds-chains-dynkcpt-nolln-hightau': Ns(
        net=ds_tree(dyn_k_cpt=True),
        hypers=ds_hypers_dynkcpt)}

################################################################################
# Parse command-line arguments.
################################################################################

parser = ArgumentParser(description=__doc__)
parser.add_argument('expt', help='the experiment to perform',
                    choices=experiments.keys())

expt_name = parser.parse_args().expt
expt = experiments[expt_name]

################################################################################
# Load the dataset.
################################################################################

dataset = Dataset('data/hybrid.npz')
# dataset = Dataset('data/mnist.npz')
# dataset = Dataset('data/cifar-10.npz')

################################################################################
# Train networks.
################################################################################

def p_cor_by_cls(net, ℓ):
    return tf.expand_dims(ℓ.p_ev * ℓ.δ_cor, 1) * net.y

def p_inc_by_cls(net, ℓ):
    return tf.expand_dims(ℓ.p_ev * (1 - ℓ.δ_cor), 1) * net.y

def state_tensors(net):
    tot_n_ops = lambda ℓ: ℓ.n_ops + getattr(ℓ.router, 'n_ops', 0)
    return {(net, 'acc'): sum(ℓ.p_ev * ℓ.δ_cor for ℓ in net.leaves),
            (net, 'moc'): sum(ℓ.p_ev * tot_n_ops(ℓ) for ℓ in net.layers),
            **{(ℓ, 'p_cor'): ℓ.p_ev * ℓ.δ_cor for ℓ in net.leaves},
            **{(ℓ, 'p_inc'): ℓ.p_ev * (1 - ℓ.δ_cor) for ℓ in net.leaves},
            **{(ℓ, 'p_cor_by_cls'): p_cor_by_cls(net, ℓ) for ℓ in net.leaves},
            **{(ℓ, 'p_inc_by_cls'): p_inc_by_cls(net, ℓ) for ℓ in net.leaves},
            **{(ℓ, 'p_tr'): ℓ.p_tr for ℓ in net.leaves if hasattr(ℓ, 'p_tr')},
            **{(ℓ, 'x_rte'): tf.reduce_mean(tf.abs(ℓ.router.x), 1)
               for ℓ in net.layers if hasattr(ℓ.router, 'x')},
            **{(ℓ, 'c_err'): ℓ.c_err for ℓ in net.leaves},
            **{(ℓ, 'c_err_cor'): ℓ.c_err_cor for ℓ in net.leaves
               if hasattr(ℓ, 'c_err_cor')}}

def train_net():
    expt = experiments[expt_name]
    net = expt.net()
    net_state = state_tensors(net)
    tf.initialize_all_variables().run()
    for t in range(n_iter):
        x0, y = dataset.augmented_training_batch(batch_size)
        ϕ = expt.hypers(net, t)
        print('  --- Iteration %i ---\r' % (t + 1), end='', flush=True)
        net.train.run({
            net.x0: x0, net.y: y, net.mode: 'tr',
            net.λ_lrn: λ_lrn(t), **ϕ})
    makedirs('nets/%s' % expt_name, exist_ok=True)
    for i, k_cpt in enumerate(k_cpts):
        ϕ_i = {**ϕ, net.k_cpt: [k_cpt]}
        desc = net_desc(net, dataset, ϕ_i, net_state)
        np.save('nets/%s/%.4i-stats.npy' % (expt_name, i), desc)
    write_net('nets/%s/net.npy' % expt_name, net)
    print()

with tf.Graph().as_default():
    sess = tf.Session(config=tf.ConfigProto(
        gpu_options=tf.GPUOptions(allow_growth=True)))
    with sess.as_default():
        train_net()
