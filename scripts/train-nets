#!/usr/bin/env python3
'''
Train statically- or dynamically-routed networks.
'''
from argparse import ArgumentParser
from os import makedirs
from types import SimpleNamespace as Ns

import numpy as np
import numpy.random as rand
import tensorflow as tf

from lib.data import Dataset
from lib.serdes import write_net
from arch_and_hypers import (
    arch, batch_size, cr_chain, cr_tree, ds_chain, ds_tree, k_cpts, n_iter,
    sr_chain, t_log, λ_lrn, τ_cr, τ_ds)

################################################################################
# Define experiments.
################################################################################

sr_hypers = lambda net, t: {}
cr_hypers = lambda net, t: {net.τ: τ_cr(t)}
ds_hypers = lambda net, t: {net.τ: τ_ds(t)}
ds_hypers_nokdec = lambda net, t: {net.τ: τ_ds(t), net.k_dec: 0}
ds_hypers_dynkcpt = lambda net, t: {
    net.τ: τ_ds(t), net.k_cpt: rand.choice(k_cpts, batch_size)}

experiments = {
    'sr-chains-mnist-nolln': Ns(
        nets=[sr_chain(n) for n in range(1, len(arch) + 1)],
        hypers=sr_hypers),
    'ds-chains-mnist-nolln-hightau': Ns(
        nets=[ds_chain(k_cpt=k) for k in k_cpts],
        hypers=ds_hypers),
    'ds-chains-notalr-nolln-hightau': Ns(
        nets=[ds_chain(k_cpt=k, talr=False) for k in k_cpts],
        hypers=ds_hypers),
    'ds-chains-dynkcpt-nolln-hightau': Ns(
        nets=[ds_tree(dyn_k_cpt=True)],
        hypers=ds_hypers_dynkcpt),
    'ds-trees-nolln-hightau': Ns(
        nets=[ds_tree(k_cpt=k) for k in k_cpts],
        hypers=ds_hypers),
    'cr-chains-nolln-hightau': Ns(
        nets=[cr_chain(k_cpt=k) for k in k_cpts],
        hypers=cr_hypers),
    'cr-chains-clserr-nolln-hightau': Ns(
        nets=[cr_chain(k_cpt=k, use_cls_err=True) for k in k_cpts],
        hypers=cr_hypers),
    'cr-chains-notalr-nolln-hightau': Ns(
        nets=[cr_chain(k_cpt=k, talr=False) for k in k_cpts],
        hypers=cr_hypers),
    'cr-trees-nolln-hightau': Ns(
        nets=[cr_tree(k_cpt=k) for k in k_cpts],
        hypers=cr_hypers),
    'cr-chains-opt-nolln-hightau': Ns(
        nets=[cr_chain(k_cpt=k, optimistic=True) for k in k_cpts],
        hypers=cr_hypers),
    'cr-chains-opt-clserr-nolln-hightau': Ns(
        nets=[cr_chain(k_cpt=k, optimistic=True, use_cls_err=True)
              for k in k_cpts],
        hypers=cr_hypers),
    'cr-chains-opt-notalr-nolln-hightau': Ns(
        nets=[cr_chain(k_cpt=k, optimistic=True, talr=False)
              for k in k_cpts],
        hypers=cr_hypers),
    'cr-trees-opt-nolln-hightau': Ns(
        nets=[cr_tree(k_cpt=k, optimistic=True) for k in k_cpts],
        hypers=cr_hypers)}

################################################################################
# Parse command-line arguments.
################################################################################

parser = ArgumentParser(description=__doc__)
parser.add_argument('expt', help='the experiment to perform',
                    choices=experiments.keys())

expt_name = parser.parse_args().expt
expt = experiments[expt_name]

################################################################################
# Load the dataset.
################################################################################

dataset = Dataset('data/hybrid.npz')
# dataset = Dataset('data/mnist.npz')
# dataset = Dataset('data/cifar-10.npz')

################################################################################
# Train networks.
################################################################################

def p_cor_by_cls(net, ℓ):
    return tf.expand_dims(ℓ.p_ev * ℓ.δ_cor, 1) * net.y

def p_inc_by_cls(net, ℓ):
    return tf.expand_dims(ℓ.p_ev * (1 - ℓ.δ_cor), 1) * net.y

def state_tensors(net):
    tot_n_ops = lambda ℓ: ℓ.n_ops + getattr(ℓ.router, 'n_ops', 0)
    return {(net, 'acc'): sum(ℓ.p_ev * ℓ.δ_cor for ℓ in net.leaves),
            (net, 'moc'): sum(ℓ.p_ev * tot_n_ops(ℓ) for ℓ in net.layers),
            **{(ℓ, 'p_cor'): ℓ.p_ev * ℓ.δ_cor for ℓ in net.leaves},
            **{(ℓ, 'p_inc'): ℓ.p_ev * (1 - ℓ.δ_cor) for ℓ in net.leaves},
            **{(ℓ, 'p_cor_by_cls'): p_cor_by_cls(net, ℓ) for ℓ in net.leaves},
            **{(ℓ, 'p_inc_by_cls'): p_inc_by_cls(net, ℓ) for ℓ in net.leaves},
            **{(ℓ, 'p_tr'): ℓ.p_tr for ℓ in net.leaves if hasattr(ℓ, 'p_tr')},
            **{(ℓ, 'x_rte'): tf.reduce_mean(tf.abs(ℓ.router.x), 1)
               for ℓ in net.layers if hasattr(ℓ.router, 'x')},
            **{(ℓ, 'c_err'): ℓ.c_err for ℓ in net.leaves},
            **{(ℓ, 'c_err_cor'): ℓ.c_err_cor for ℓ in net.leaves
               if hasattr(ℓ, 'c_err_cor')}}

def train_net(i):
    expt = experiments[expt_name]
    net = expt.nets[i]()
    net_state = state_tensors(net)
    tf.initialize_all_variables().run()
    for t in range(n_iter):
        x0, y = dataset.augmented_training_batch(batch_size)
        ϕ = expt.hypers(net, t)
        print('  --- Iteration %i ---\r' % (t + 1), end='', flush=True)
        net.train.run({
            net.x0: x0, net.y: y, net.mode: 'tr',
            net.λ_lrn: λ_lrn(t), **ϕ})
        if (t + 1) % t_log == 0:
            desc = net_desc(net, dataset, ϕ, net_state)
            text = render_net_desc(desc, (
                'nets/%s/%.4i.npy — Epoch %i'
                % (expt_name, i, t + 1)))
            makedirs('nets/%s' % expt_name, exist_ok=True)
            makedirs('nets/%s/%.4i-stats' % (expt_name, i), exist_ok=True)
            np.save('nets/%s/%.4i-stats/%.8i.npy' % (expt_name, i, t + 1), desc)
            np.save('nets/%s/%.4i-stats.npy' % (expt_name, i), desc)
            with open('nets/%s/%.4i-log.txt' % (expt_name, i), 'a+') as f:
                f.write(text + '\n')
            print(text)
    makedirs('nets/%s' % expt_name, exist_ok=True)
    write_net('nets/%s/%.4i.npy' % (expt_name, i), net)

for i in range(len(expt.nets)):
    with tf.Graph().as_default():
        sess = tf.Session(config=tf.ConfigProto(
            gpu_options=tf.GPUOptions(allow_growth=True)))
        with sess.as_default():
            train_net(i)
