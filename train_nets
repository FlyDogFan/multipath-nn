#!/usr/bin/env python3
import numpy as np
import tensorflow as tf

from lib.data import Dataset
from lib.nets import (
    CRRouting, DSRouting, LogReg, Net, ReConv, ReConvMP, ReLin,
    SmartCRRouting, SmartDSRouting)

################################################################################
# Load data.
################################################################################

dataset = Dataset('data/mnist.mat')
x0_shape = dataset.x0_shape
y_shape = dataset.y_shape
n_cls = y_shape[0]

################################################################################
# Define logging functions.
################################################################################

def mean_over(batches, sess, net, func):
    sums = np.zeros(len(list(net.layers)), object)
    count = 0
    for x0, y in batches:
        stats = sess.run(list(map(func, net.layers)), {net.x0: x0, net.y: y})
        for i in range(len(sums)):
            sums[i] += np.sum(stats[i], 0)
        count += len(x0)
    return {l: s / count for l, s in zip(net.layers, sums)}

def describe(layer, λ_cor, λ_inc):
    if len(layer.sinks) == 0:
        return (
            layer.__class__.__name__
            + ' [%.1f%% ✓ %.1f%% ×]'
            % (100 * λ_cor[layer], 100 * λ_inc[layer]))
    else:
        return (
            layer.__class__.__name__
            + ''.join(
                '\n↳ ' + describe(s, λ_cor, λ_inc).replace(
                    '\n', '\n| ' if i < len(layer.sinks) - 1 else '\n  ')
                for i, s in enumerate(layer.sinks)))

def log_progress(sess, net, t, batch_size=512):
    def p_cor(layer):
        if len(layer.sinks) == 0:
            δ_cor = tf.equal(tf.argmax(layer.x, 1), tf.argmax(net.y, 1))
            return layer.p_ev * tf.to_float(δ_cor)
        else:
            return tf.zeros(tf.shape(layer.x)[:1])
    def p_inc(layer):
        if len(layer.sinks) == 0:
            δ_inc = tf.not_equal(tf.argmax(layer.x, 1), tf.argmax(net.y, 1))
            return layer.p_ev * tf.to_float(δ_inc)
        else:
            return tf.zeros(tf.shape(layer.x)[:1])
    λ_cor = mean_over(dataset.test_batches(batch_size), sess, net, p_cor)
    λ_inc = mean_over(dataset.test_batches(batch_size), sess, net, p_inc)
    print('····························································')
    print(' Epoch %i' % t)
    print('····························································')
    print(('⋮\n%s\n' % describe(net.root, λ_cor, λ_inc))
              .replace('\n', '\n⋮   '))

################################################################################
# Define a trining function.
################################################################################

def train(net, batch_size=64, n_epochs=100, logging_period=5):
    train_op = tf.train.AdamOptimizer().minimize(tf.reduce_mean(net.ℓ_tr))
    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())
        for t in range(n_epochs):
            if t % logging_period == 0:
                log_progress(sess, net, t)
            for x0, y in dataset.training_batches(batch_size):
                train_op.run({net.x0: x0, net.y: y})
        log_progress(sess, net, 100)

################################################################################
# Train networks.
################################################################################

k_cpt = 0e-7
k_l2 = 1e-5

# train(Net(x0_shape, y_shape,
#     ReConvMP(32, 2, 5, k_cpt, k_l2,
#         ReConvMP(64, 2, 5, k_cpt, k_l2,
#             ReLin(128, k_cpt, k_l2, LogReg(n_cls, k_l2))))))

# train(Net(x0_shape, y_shape, DSRouting(0.5, LogReg(n_cls, k_l2),
#     ReConvMP(32, 2, 5, k_cpt, k_l2, DSRouting(0.5, LogReg(n_cls, k_l2),
#         ReConvMP(64, 2, 5, k_cpt, k_l2, DSRouting(0.5, LogReg(n_cls, k_l2),
#             ReLin(128, k_cpt, k_l2, LogReg(n_cls, 0)))))))))

train(Net(x0_shape, y_shape, CRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
    ReConvMP(32, 2, 5, k_cpt, k_l2, CRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
        ReConvMP(64, 2, 5, k_cpt, k_l2, CRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
            ReLin(128, k_cpt, k_l2, LogReg(n_cls, k_l2)))))))))

# train(Net(x0_shape, y_shape, SmartCRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
#     ReConvMP(32, 2, 5, k_cpt, k_l2, SmartCRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
#         ReConvMP(64, 2, 5, k_cpt, k_l2, SmartCRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
#             ReLin(128, k_cpt, k_l2, LogReg(n_cls, k_l2)))))))))

# train(Net(x0_shape, y_shape, CRRouting(0.1, 0.5, LogReg(n_cls, 0),
#     ReConvMP(32, 2, 5, k_cpt, 0, CRRouting(0.1, 0.5, LogReg(n_cls, 0),
#         ReConvMP(64, 2, 5, k_cpt, 0, LogReg(n_cls, 0)),
#         ReConvMP(64, 2, 5, k_cpt, 0, LogReg(n_cls, 0)))),
#     ReConvMP(32, 2, 5, k_cpt, 0, CRRouting(0.1, 0.5, LogReg(n_cls, 0),
#         ReConvMP(64, 2, 5, k_cpt, 0, LogReg(n_cls, 0)),
#         ReConvMP(64, 2, 5, k_cpt, 0, LogReg(n_cls, 0)))))))

# train(Net(x0_shape, y_shape, SmartCRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
#     ReConvMP(32, 2, 5, k_cpt, k_l2, SmartCRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
#         ReConvMP(64, 2, 5, k_cpt, k_l2, LogReg(n_cls, k_l2)),
#         ReConvMP(64, 2, 5, k_cpt, k_l2, LogReg(n_cls, k_l2)))),
#     ReConvMP(32, 2, 5, k_cpt, k_l2, SmartCRRouting(0.1, 0.5, LogReg(n_cls, k_l2),
#         ReConvMP(64, 2, 5, k_cpt, k_l2, LogReg(n_cls, k_l2)),
#         ReConvMP(64, 2, 5, k_cpt, k_l2, LogReg(n_cls, k_l2)))))))
